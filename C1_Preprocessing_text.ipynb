{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e23211",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a403467a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/the-verdict.txt', <http.client.HTTPMessage at 0x7f8cd0db9c70>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "filepath = \"./data/the-verdict.txt\"\n",
    "\n",
    "urllib.request.urlretrieve(url, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb71eb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total Characters:\", len(raw_text))\n",
    "print(raw_text[0:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c043db",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371c02a3",
   "metadata": {},
   "source": [
    "### Basic Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78782ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', ' ', 'world!', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world! This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98776421",
   "metadata": {},
   "source": [
    "## handling punctuations as a separate token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adab8473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', ' world', '!', ' This', ',', ' is a test', '.', '']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\|\\s)', text)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ba8d5b",
   "metadata": {},
   "source": [
    "# remove whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50a8aaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'world', '!', 'This', ',', 'is a test', '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = [item.strip() for item in result  if item.strip()]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7611529",
   "metadata": {},
   "source": [
    "## Preprocessing raw text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd907223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 4690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'HAD',\n",
       " 'always',\n",
       " 'thought',\n",
       " 'Jack',\n",
       " 'Gisburn',\n",
       " 'rather',\n",
       " 'a',\n",
       " 'cheap',\n",
       " 'genius']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed  if item.strip()]\n",
    "print(\"Total tokens:\", len(preprocessed))\n",
    "preprocessed[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba5dabc",
   "metadata": {},
   "source": [
    "# Convert token into token IDs\n",
    "\n",
    "This is an intermediate step before converting the tokenID into embedding vectors.\n",
    "To map the the previously generated tokens into token IDs, we have to build the vocabulary first. \n",
    "This vocabulary defines how we map each unique word and special character to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7cfe8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 1130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('!', 0),\n",
       " ('\"', 1),\n",
       " (\"'\", 2),\n",
       " ('(', 3),\n",
       " (')', 4),\n",
       " (',', 5),\n",
       " ('--', 6),\n",
       " ('.', 7),\n",
       " (':', 8),\n",
       " (';', 9),\n",
       " ('?', 10),\n",
       " ('A', 11),\n",
       " ('Ah', 12),\n",
       " ('Among', 13),\n",
       " ('And', 14),\n",
       " ('Are', 15),\n",
       " ('Arrt', 16),\n",
       " ('As', 17),\n",
       " ('At', 18),\n",
       " ('Be', 19),\n",
       " ('Begin', 20),\n",
       " ('Burlington', 21),\n",
       " ('But', 22),\n",
       " ('By', 23),\n",
       " ('Carlo', 24),\n",
       " ('Chicago', 25),\n",
       " ('Claude', 26),\n",
       " ('Come', 27),\n",
       " ('Croft', 28),\n",
       " ('Destroyed', 29),\n",
       " ('Devonshire', 30),\n",
       " ('Don', 31),\n",
       " ('Dubarry', 32),\n",
       " ('Emperors', 33),\n",
       " ('Florence', 34),\n",
       " ('For', 35),\n",
       " ('Gallery', 36),\n",
       " ('Gideon', 37),\n",
       " ('Gisburn', 38),\n",
       " ('Gisburns', 39),\n",
       " ('Grafton', 40),\n",
       " ('Greek', 41),\n",
       " ('Grindle', 42),\n",
       " ('Grindles', 43),\n",
       " ('HAD', 44),\n",
       " ('Had', 45),\n",
       " ('Hang', 46),\n",
       " ('Has', 47),\n",
       " ('He', 48),\n",
       " ('Her', 49)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "list(vocab.items())[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713d2382",
   "metadata": {},
   "source": [
    "Putting it now all together into a tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b5ace51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "\n",
    "    def __init__(self, vocab: dict):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)            \n",
    "        tokens = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in tokens]\n",
    "        return tokens, ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c0e3af",
   "metadata": {},
   "source": [
    "* The encode function turns text into token IDs\n",
    "* The decode function turns token IDs back into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9c8a3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"It's the last he painted, you know,\" \n",
      "           Mrs. Gisburn said with pardonable pride.\n",
      "Tokens: ['\"', 'It', \"'\", 's', 'the', 'last', 'he', 'painted', ',', 'you', 'know', ',', '\"', 'Mrs', '.', 'Gisburn', 'said', 'with', 'pardonable', 'pride', '.']\n",
      "Encoded: [1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "Decoded: \" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "print(text)\n",
    "tokens, ids = tokenizer.encode(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Encoded:\", ids)\n",
    "print(\"Decoded:\", tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2ece15",
   "metadata": {},
   "source": [
    "## Adding Special context Tokens\n",
    "It's useful to add some \"special\" tokens for unknown words and to denote the end of a text\n",
    "\n",
    "* Some tokenizers use special tokens to help the LLM with additional context\n",
    "\n",
    "* Some of these special tokens are\n",
    "\n",
    "    * [BOS] (beginning of sequence) marks the beginning of text\n",
    "    * [EOS] (end of sequence) marks where the text ends (this is usually used to concatenate multiple unrelated texts, e.g., two different Wikipedia articles or two    different books, and so on)\n",
    "    *   [PAD] (padding) if we train LLMs with a batch size greater than 1 (we may include multiple texts with different lengths; with the padding token we pad the  shorter texts to the longest length so that all texts have an equal length)\n",
    "    * [UNK] to represent words that are not included in the vocabulary\n",
    "\n",
    "* Note that GPT-2 does not need any of these tokens mentioned above but only uses an <|endoftext|> token to reduce complexity\n",
    "\n",
    "* The <|endoftext|> is analogous to the [EOS] token mentioned above\n",
    "\n",
    "* GPT also uses the <|endoftext|> for padding (since we typically use a mask when training on batched inputs, we would not attend padded tokens anyways, so it does not matter what these tokens are)\n",
    "\n",
    "* GPT-2 does not use an <UNK> token for out-of-vocabulary words; instead, GPT-2 uses a byte-pair encoding (BPE) tokenizer, which breaks down words into subword units which we will discuss in a later section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06422e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc51afd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168e606e",
   "metadata": {},
   "source": [
    "We also need to adjust the tokenizer accordingly so that it knows when and how to use the new <unk> token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "831571e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86a0e280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)\n",
    "print(tokenizer.encode(text))\n",
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b433af95",
   "metadata": {},
   "source": [
    "## BytePair Encoding (BPE)\n",
    "\n",
    "BPE tokenizers break down unknown words into subwords and individual characters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4837b3df",
   "metadata": {},
   "source": [
    "* GPT-2 used BytePair encoding (BPE) as its tokenizer\n",
    "* it allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words\n",
    "* For instance, if GPT-2's vocabulary doesn't have the word \"unfamiliarword,\" it might tokenize it as [\"unfam\", \"iliar\", \"word\"] or some other subword breakdown, depending on its trained BPE merges\n",
    "* The original BPE tokenizer can be found here: https://github.com/openai/gpt-2/blob/master/src/encoder.py\n",
    "* In this chapter, we are using the BPE tokenizer from OpenAI's open-source tiktoken library, which implements its core algorithms in Rust to improve computational performance\n",
    "* I created a notebook in the ./bytepair_encoder that compares these two implementations side-by-side (tiktoken was about 5x faster on the sample text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4748e43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.7.0-cp38-cp38-macosx_10_9_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/rathods/opt/anaconda3/envs/tti-model-inference-env/lib/python3.8/site-packages (from tiktoken) (2023.5.5)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/rathods/opt/anaconda3/envs/tti-model-inference-env/lib/python3.8/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rathods/opt/anaconda3/envs/tti-model-inference-env/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rathods/opt/anaconda3/envs/tti-model-inference-env/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rathods/opt/anaconda3/envs/tti-model-inference-env/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rathods/opt/anaconda3/envs/tti-model-inference-env/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
      "Downloading tiktoken-0.7.0-cp38-cp38-macosx_10_9_x86_64.whl (961 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.0/962.0 kB\u001b[0m \u001b[31m387.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.7.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f7de482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "#print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ecb0d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2', 'r50k_base', 'p50k_base', 'p50k_edit', 'cl100k_base', 'o200k_base']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken.list_encoding_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bdb04f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9353762a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 2954, 261, 675, 5372, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunkonwnplace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2204fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunkonwnplace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44844cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Vocabulary Size:', 50257)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Vocabulary Size:\", tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e269fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_token_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1999c3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[461, 86, 343, 86, 9310, 73, 69, 375, 71, 220, 959]\n",
      "akwirwdsjfodh ier\n"
     ]
    }
   ],
   "source": [
    "# lets try how BPE handles unknown word: \"akwirwdsjfodh ier\"\n",
    "\n",
    "text = \"akwirwdsjfodh ier\"\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(token_ids)\n",
    "print(tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b628f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~������'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(list(range(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5782ec",
   "metadata": {},
   "source": [
    "# Data sampling with a sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc58eac",
   "metadata": {},
   "source": [
    "We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21bb13aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06f4ec",
   "metadata": {},
   "source": [
    "* For each text chunk, we want the inputs and targets\n",
    "* Since we want the model to predict the next word, the targets are the inputs shifted by one position to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9c2be8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([40, 367, 2885, 1464, 1807], 'I HAD always thought')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_text[:5], tokenizer.decode(enc_text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "271732f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "# we remove first 50 tokens for demonstration purpose\n",
    "\n",
    "enc_sample = enc_text[50:]\n",
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1: context_size + 1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0affe0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] --> 4920\n",
      " and -->  established\n",
      "---\n",
      "[290, 4920] --> 2241\n",
      " and established -->  himself\n",
      "---\n",
      "[290, 4920, 2241] --> 287\n",
      " and established himself -->  in\n",
      "---\n",
      "[290, 4920, 2241, 287] --> 257\n",
      " and established himself in -->  a\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    target = enc_sample[i]\n",
    "    print(context, \"-->\", target)\n",
    "    print(tokenizer.decode(context), \"-->\", tokenizer.decode([target]))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5780b30f",
   "metadata": {},
   "source": [
    "* We will take care of the next-word prediction in a later chapter after we covered the attention mechanism\n",
    "* For now, we implement a simple data loader that iterates over the input dataset and returns the inputs and targets shifted by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d120ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class GPTDatasetV1(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(text)\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            x = token_ids[i: i + max_length]\n",
    "            y = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(x))\n",
    "            self.target_ids.append(torch.tensor(y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2ebcf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4955408",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81ef70b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c2f3375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d8074",
   "metadata": {},
   "source": [
    "* We can also create batched outputs\n",
    "* Note that we increase the stride here so that we don't have overlaps between the batches, since more overlap could lead to increased overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5958fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55e3a0e",
   "metadata": {},
   "source": [
    "## Create Token Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507ba50c",
   "metadata": {},
   "source": [
    "* The data is already almost ready for an LLM\n",
    "* But lastly let us embed the tokens in a continuous vector representation using an embedding layer\n",
    "* Usually, these embedding layers are part of the LLM itself and are updated (trained) during model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b8dafd6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3374, -0.1778, -0.1690],\n",
       "        [ 0.9178,  1.5810,  1.3010],\n",
       "        [ 1.2753, -0.2010, -0.1606],\n",
       "        [-0.4015,  0.9666, -1.1481],\n",
       "        [-1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Suppose we have the following four input examples with input ids 2, 3, 5, and 1 (after tokenization):\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "\n",
    "#For the sake of simplicity, suppose we have a small vocabulary of only 6 words and we want to create embeddings of size 3:\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "#This would result in a 6x3 weight matrix:\n",
    "embedding_layer.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8388d882",
   "metadata": {},
   "source": [
    "* For those who are familiar with one-hot encoding, the embedding layer approach above is essentially just a more efficient way of implementing one-hot encoding followed by matrix multiplication in a fully-connected layer, which is described in the supplementary code in ./embedding_vs_matmul\n",
    "\n",
    "* Because the embedding layer is just a more efficient implementation that is equivalent to the one-hot encoding and matrix-multiplication approach it can be seen as a neural network layer that can be optimized via backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eaf764e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To convert a token with id 3 into a 3-dimensional vector, we do the following:\n",
    "embedding_layer(torch.tensor([3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed44408b",
   "metadata": {},
   "source": [
    "* An embedding layer is essentially a look-up operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39726c53",
   "metadata": {},
   "source": [
    "## Encoding Word Positions\n",
    "\n",
    "* Embedding layer convert IDs into identical vector representations regardless of where they are located in the input sequence.\n",
    "* Positional embeddings are combined with the token embedding vector to form the input embeddings for a large language model.\n",
    "* The BytePair encoder has a vocabulary size of 50,257\n",
    "* Suppose we want to encode the input tokens into a 256-dimensional vector representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39ade0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "762d64e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f5b4215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e11b94fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)\n",
    "\n",
    "# uncomment & execute the following line to see how the embeddings look like\n",
    "#print(token_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991e0696",
   "metadata": {},
   "source": [
    "* GPT-2 uses absolute position embeddings, so we just create another embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06a25b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "# uncomment & execute the following line to see how the embedding layer weights look like\n",
    "# print(pos_embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a42f775b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "50e93e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905125e7",
   "metadata": {},
   "source": [
    "* To create the input embeddings used in an LLM, we simply add the token and the positional embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1b1b8618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a5c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tti-model-inference-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
