{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a84621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size:int = 4, dim_out: int = 4):\n",
    "        super().__init__()\n",
    "        self.wq = torch.nn.Linear(embedding_size, dim_out, bias=False)\n",
    "        self.wk = torch.nn.Linear(embedding_size, dim_out, bias=False)\n",
    "        self.wv = torch.nn.Linear(embedding_size, dim_out, bias=False)\n",
    "        self.dim_out = dim_out\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "\n",
    "        query = self.wq(embeddings)\n",
    "        key = self.wk(embeddings)\n",
    "        value = self.wv(embeddings)\n",
    "\n",
    "        sims = torch.matmul(query, key.T)\n",
    "        scaled_sim = torch.softmax(sims / (self.dim_out ** 0.5), dim=1)\n",
    "\n",
    "        return torch.matmul(scaled_sim, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a71096f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1002, -0.6092, -0.9798, -1.6091],\n",
       "        [-0.7121,  0.3037, -0.7773, -0.2515],\n",
       "        [-0.2223,  1.6871,  0.2284,  0.4676]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "embeddings = torch.nn.Embedding(1000, embedding_dim=4)\n",
    "attention = SelfAttention(embedding_size=4, dim_out=4)\n",
    "\n",
    "token_ids = torch.tensor([1, 2, 3])\n",
    "encodings = embeddings(token_ids)\n",
    "\n",
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5817215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3959, -0.0747, -0.1569, -0.0704],\n",
       "        [ 0.3421, -0.0247, -0.1320, -0.1292],\n",
       "        [ 0.2429,  0.0757, -0.0831, -0.2359]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbf70c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "attention = SelfAttention(embedding_size=2, dim_out=2)\n",
    "encodings = torch.tensor([[1.16, 0.23],\n",
    "                          [0.57, 1.36],\n",
    "                          [4.41, -2.16]])\n",
    "\n",
    "attention(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d478b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5406, -0.1657],\n",
       "        [ 0.5869,  0.6496]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.wq.weight.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c37c738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1549, -0.3443],\n",
       "        [ 0.1427,  0.4153]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.wk.weight.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc1ac730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6233,  0.6146],\n",
       "        [-0.5188,  0.1323]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.wv.weight.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6d8aefb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7621, -0.0428],\n",
       "        [ 1.1063,  0.7890],\n",
       "        [ 1.1164, -2.1336]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = attention.wq(encodings)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac80c199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1469, -0.3038],\n",
       "        [ 0.1057,  0.3685],\n",
       "        [-0.9914, -2.4152]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = attention.wk(encodings)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc37f6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0990,  0.0648, -0.6523],\n",
       "        [-0.4022,  0.4078, -3.0024],\n",
       "        [ 0.4842, -0.6683,  4.0461]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = torch.matmul(q, k.T)\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd62f4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3573, 0.4011, 0.2416],\n",
       "        [0.3410, 0.6047, 0.0542],\n",
       "        [0.0722, 0.0320, 0.8959]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_sim = F.softmax(sims / (torch.tensor(2) ** 0.5), dim=1)\n",
    "scaled_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "def176d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(scaled_sim, attention.wv(encodings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ded097",
   "metadata": {},
   "source": [
    "## MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d6ec78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size:int = 4, dim_out:int = 4, num_heads: int = 4):\n",
    "        super().__init__()\n",
    "        self.heads = torch.nn.ModuleList([SelfAttention(embedding_size, dim_out) for _ in range(num_heads) \n",
    "                                          ])\n",
    "    def forward(self, embeddings):\n",
    "        return torch.cat([head(embeddings) for head in self.heads], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e8a9ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0100,  1.0641, -0.7081, -0.8268],\n",
       "        [ 0.2040,  0.7057, -0.7417, -0.9193],\n",
       "        [ 3.4989,  2.2427, -0.7190, -0.8447]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "attention = MultiHeadAttention(embedding_size=2, dim_out=2, num_heads=2)\n",
    "encodings = torch.tensor([[1.16, 0.23],\n",
    "                          [0.57, 1.36],\n",
    "                          [4.41, -2.16]])\n",
    "\n",
    "attention(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46c7ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a95cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tti-model-inference-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
