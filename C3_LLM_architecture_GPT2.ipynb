{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9682638",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "090008da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MyTransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm1 = torch.nn.LayerNorm(config[\"emb_dim\"])\n",
    "        \n",
    "        self.attention = torch.nn.MultiheadAttention(embed_dim=config[\"emb_dim\"],\n",
    "                                                     num_heads=config[\"n_heads\"],\n",
    "                                                     dropout=config[\"drop_rate\"],\n",
    "                                                     bias=config[\"qkv_bias\"],\n",
    "                                                     add_bias_kv=config[\"qkv_bias\"],\n",
    "                                                     batch_first=True\n",
    "                                                     )\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(config[\"drop_rate\"])\n",
    "        self.norm2 = torch.nn.LayerNorm(config[\"emb_dim\"])\n",
    "        \n",
    "        self.feedforward = torch.nn.Sequential(\n",
    "            torch.nn.Linear(config[\"emb_dim\"], 4 * config[\"emb_dim\"]),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(4 * config[\"emb_dim\"], config[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, seq, emb = x.shape\n",
    "\n",
    "        x_shortcut = x\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        mask = torch.nn.Transformer.generate_square_subsequent_mask(seq)\n",
    "        attn_output, attn_output_weights = self.attention(query=x, key=x, value=x, is_causal=True, attn_mask=mask)\n",
    "        \n",
    "        x = self.dropout(attn_output)\n",
    "\n",
    "        x = x + x_shortcut\n",
    "        x_shortcut = x\n",
    "\n",
    "        x = self.norm2(x)\n",
    "        x = self.feedforward(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x + x_shortcut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52a25591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([2, 4, 768])\n",
      "Output Shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = MyTransformerBlock(GPT_CONFIG)\n",
    "output = block(x)\n",
    "print(\"Input Shape:\", x.shape)\n",
    "print(\"Output Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cfabb5",
   "metadata": {},
   "source": [
    "## GPT2 From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3798ec04",
   "metadata": {},
   "source": [
    "* GPT-2 small = 124 million params\n",
    "* GPT-2 Medium = 1024-dimensional embeddings, 24 transformer blocks, 16-multi-head attention heads\n",
    "* GPT-2 large = 1280-dimensional embeddings, 36 transformer blocks, 20-multi-head attention heads\n",
    "* GPT-2 XL = 1600-dimensional embeddings, 48 transformer blocks, 25-multi-head attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed7b4f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = torch.nn.Embedding(config['vocab_size'], config[\"emb_dim\"])\n",
    "        self.position_embedding = torch.nn.Embedding(config['context_length'], config[\"emb_dim\"])\n",
    "        self.drop_emb = torch.nn.Dropout(config[\"drop_rate\"])\n",
    "\n",
    "        self.transformer_blocks = torch.nn.Sequential(\n",
    "            *[MyTransformerBlock(config) for _ in range(config[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.layer_norm = torch.nn.LayerNorm(config[\"emb_dim\"])\n",
    "        self.out_head = torch.nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        \n",
    "        batch_size, seq_len = x.shape   #[BS, CONTEXT_LENGTH]\n",
    "\n",
    "        token_embeds = self.token_embedding(x)\n",
    "        position_embeds = self.position_embedding(torch.arange(seq_len, device=x.device))\n",
    "\n",
    "        x = token_embeds + position_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "261410b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'gpt2'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc15fce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[6109, 3626, 6100,  345],\n",
       "         [6109, 1110, 6622,  257]]),\n",
       " torch.Size([2, 4]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch = torch.tensor([ \n",
    "                tokenizer.encode(\"Every effort moves you\"),\n",
    "                tokenizer.encode(\"Every day holds a\"),\n",
    "              ])\n",
    "input_batch, input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8895635a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([2, 4, 768])\n",
      "Output Shape: torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG)\n",
    "output = model(input_batch)\n",
    "print(\"Input Shape:\", x.shape)\n",
    "print(\"Output Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59dfc02",
   "metadata": {},
   "source": [
    "* GPT-2 model outputs [batch_size, num_token, vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee08d130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "GPTModel                                 [2, 4, 50257]             --\n",
       "├─Embedding: 1-1                         [2, 4, 768]               38,597,376\n",
       "├─Embedding: 1-2                         [4, 768]                  786,432\n",
       "├─Dropout: 1-3                           [2, 4, 768]               --\n",
       "├─Sequential: 1-4                        [2, 4, 768]               --\n",
       "│    └─MyTransformerBlock: 2-1           [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-1               [2, 4, 768]               1,536\n",
       "│    │    └─MultiheadAttention: 3-2      [2, 4, 768]               2,359,296\n",
       "│    │    └─Dropout: 3-3                 [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-4               [2, 4, 768]               1,536\n",
       "│    │    └─Sequential: 3-5              [2, 4, 768]               4,722,432\n",
       "│    │    └─Dropout: 3-6                 [2, 4, 768]               --\n",
       "│    └─MyTransformerBlock: 2-2           [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-7               [2, 4, 768]               1,536\n",
       "│    │    └─MultiheadAttention: 3-8      [2, 4, 768]               2,359,296\n",
       "│    │    └─Dropout: 3-9                 [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-10              [2, 4, 768]               1,536\n",
       "│    │    └─Sequential: 3-11             [2, 4, 768]               4,722,432\n",
       "│    │    └─Dropout: 3-12                [2, 4, 768]               --\n",
       "│    └─MyTransformerBlock: 2-3           [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-13              [2, 4, 768]               1,536\n",
       "│    │    └─MultiheadAttention: 3-14     [2, 4, 768]               2,359,296\n",
       "│    │    └─Dropout: 3-15                [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-16              [2, 4, 768]               1,536\n",
       "│    │    └─Sequential: 3-17             [2, 4, 768]               4,722,432\n",
       "│    │    └─Dropout: 3-18                [2, 4, 768]               --\n",
       "│    └─MyTransformerBlock: 2-4           [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-19              [2, 4, 768]               1,536\n",
       "│    │    └─MultiheadAttention: 3-20     [2, 4, 768]               2,359,296\n",
       "│    │    └─Dropout: 3-21                [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-22              [2, 4, 768]               1,536\n",
       "│    │    └─Sequential: 3-23             [2, 4, 768]               4,722,432\n",
       "│    │    └─Dropout: 3-24                [2, 4, 768]               --\n",
       "│    └─MyTransformerBlock: 2-5           [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-25              [2, 4, 768]               1,536\n",
       "│    │    └─MultiheadAttention: 3-26     [2, 4, 768]               2,359,296\n",
       "│    │    └─Dropout: 3-27                [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-28              [2, 4, 768]               1,536\n",
       "│    │    └─Sequential: 3-29             [2, 4, 768]               4,722,432\n",
       "│    │    └─Dropout: 3-30                [2, 4, 768]               --\n",
       "│    └─MyTransformerBlock: 2-6           [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-31              [2, 4, 768]               1,536\n",
       "│    │    └─MultiheadAttention: 3-32     [2, 4, 768]               2,359,296\n",
       "│    │    └─Dropout: 3-33                [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-34              [2, 4, 768]               1,536\n",
       "│    │    └─Sequential: 3-35             [2, 4, 768]               4,722,432\n",
       "│    │    └─Dropout: 3-36                [2, 4, 768]               --\n",
       "│    └─MyTransformerBlock: 2-7           [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-37              [2, 4, 768]               1,536\n",
       "│    │    └─MultiheadAttention: 3-38     [2, 4, 768]               2,359,296\n",
       "│    │    └─Dropout: 3-39                [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-40              [2, 4, 768]               1,536\n",
       "│    │    └─Sequential: 3-41             [2, 4, 768]               4,722,432\n",
       "│    │    └─Dropout: 3-42                [2, 4, 768]               --\n",
       "│    └─MyTransformerBlock: 2-8           [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-43              [2, 4, 768]               1,536\n",
       "│    │    └─MultiheadAttention: 3-44     [2, 4, 768]               2,359,296\n",
       "│    │    └─Dropout: 3-45                [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-46              [2, 4, 768]               1,536\n",
       "│    │    └─Sequential: 3-47             [2, 4, 768]               4,722,432\n",
       "│    │    └─Dropout: 3-48                [2, 4, 768]               --\n",
       "│    └─MyTransformerBlock: 2-9           [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-49              [2, 4, 768]               1,536\n",
       "│    │    └─MultiheadAttention: 3-50     [2, 4, 768]               2,359,296\n",
       "│    │    └─Dropout: 3-51                [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-52              [2, 4, 768]               1,536\n",
       "│    │    └─Sequential: 3-53             [2, 4, 768]               4,722,432\n",
       "│    │    └─Dropout: 3-54                [2, 4, 768]               --\n",
       "│    └─MyTransformerBlock: 2-10          [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-55              [2, 4, 768]               1,536\n",
       "│    │    └─MultiheadAttention: 3-56     [2, 4, 768]               2,359,296\n",
       "│    │    └─Dropout: 3-57                [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-58              [2, 4, 768]               1,536\n",
       "│    │    └─Sequential: 3-59             [2, 4, 768]               4,722,432\n",
       "│    │    └─Dropout: 3-60                [2, 4, 768]               --\n",
       "│    └─MyTransformerBlock: 2-11          [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-61              [2, 4, 768]               1,536\n",
       "│    │    └─MultiheadAttention: 3-62     [2, 4, 768]               2,359,296\n",
       "│    │    └─Dropout: 3-63                [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-64              [2, 4, 768]               1,536\n",
       "│    │    └─Sequential: 3-65             [2, 4, 768]               4,722,432\n",
       "│    │    └─Dropout: 3-66                [2, 4, 768]               --\n",
       "│    └─MyTransformerBlock: 2-12          [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-67              [2, 4, 768]               1,536\n",
       "│    │    └─MultiheadAttention: 3-68     [2, 4, 768]               2,359,296\n",
       "│    │    └─Dropout: 3-69                [2, 4, 768]               --\n",
       "│    │    └─LayerNorm: 3-70              [2, 4, 768]               1,536\n",
       "│    │    └─Sequential: 3-71             [2, 4, 768]               4,722,432\n",
       "│    │    └─Dropout: 3-72                [2, 4, 768]               --\n",
       "├─LayerNorm: 1-5                         [2, 4, 768]               1,536\n",
       "├─Linear: 1-6                            [2, 4, 50257]             38,597,376\n",
       "==========================================================================================\n",
       "Total params: 163,000,320\n",
       "Trainable params: 163,000,320\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 270.95\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 7.47\n",
       "Params size (MB): 538.76\n",
       "Estimated Total Size (MB): 546.22\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "model = GPTModel(GPT_CONFIG)\n",
    "summary(model, input_data=input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94b92766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 163000320\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total Params:\", total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72cc8a7",
   "metadata": {},
   "source": [
    "* 163 million params\n",
    "* The original GPT-2 architecture is 124 million parameters and reuses the weights from the token embedding layer in its output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b114c355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Embedding Layer Shape: torch.Size([50257, 768])\n",
      "Output Layer Shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token Embedding Layer Shape:\", model.token_embedding.weight.shape)\n",
    "print(\"Output Layer Shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2525e235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124402944"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "163000320 - sum(param.numel() for param in model.out_head.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98008fb7",
   "metadata": {},
   "source": [
    "* the original 124-million params\n",
    "* However, using separate token embedding and output layers results in better training and model performance, hence we use separate layers in out GPT implementation. The same is true for modern LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab33034",
   "metadata": {},
   "source": [
    "### Let's compute the memory requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1273f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size of the model (MB): 621.796875\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4   # assuming each parameter is 32-bit float, hence 4 bytes\n",
    "total_size_mb = total_size_bytes / (1024 * 1024) # KB to MB\n",
    "print(\"Total Size of the model (MB):\", total_size_mb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10930785",
   "metadata": {},
   "source": [
    "### Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b9c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_sample(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]  # last vector\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c263b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [5211, 345, 760, 1997]\n",
      "tensor([[5211,  345,  760, 1997]]) torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Do you know anything\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"Encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(encoded_tensor, encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bdc665",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(encoded).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0623a258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5211,  345,  760, 1997]])\n",
      "tensor([[ 5211,   345,   760,  1997, 36255]])\n",
      "tensor([[ 5211,   345,   760,  1997, 36255, 39648]])\n",
      "tensor([[ 5211,   345,   760,  1997, 36255, 39648, 10355]])\n",
      "tensor([[ 5211,   345,   760,  1997, 36255, 39648, 10355, 17283]])\n",
      "tensor([[ 5211,   345,   760,  1997, 36255, 39648, 10355, 17283, 33705]])\n",
      "Ouput: tensor([[ 5211,   345,   760,  1997, 36255, 39648, 10355, 17283, 33705, 45834]])\n",
      "Decoded Text: Do you know anything HEL Baalalties TC img stagnation\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = generate_text_sample(model, encoded_tensor, max_new_tokens=6, context_size=GPT_CONFIG[\"context_length\"])\n",
    "\n",
    "print(\"Ouput:\", out)\n",
    "print(\"Decoded Text:\", tokenizer.decode(out.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45a1131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tti-model-inference-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
